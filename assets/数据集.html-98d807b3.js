import{_ as l,r as i,o as c,c as d,a as e,b as a,w as o,d as t,e as r}from"./app-04b0d2b0.js";const h={},p={class:"table-of-contents"},u=e("h1",{id:"图像数据集",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#图像数据集","aria-hidden":"true"},"#"),t(" 图像数据集")],-1),m=e("p",null,"test change",-1),g=e("h2",{id:"coco-microsoft-common-objects-in-context",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#coco-microsoft-common-objects-in-context","aria-hidden":"true"},"#"),t(" COCO (Microsoft Common Objects in Context)")],-1),b={href:"https://cocodataset.org/#home",target:"_blank",rel:"noopener noreferrer"},f=r('<p>The MS <strong>COCO</strong> (<strong>Microsoft Common Objects in Context</strong>) dataset is a large-scale object detection, segmentation, key-point detection, and captioning dataset. The dataset consists of 328K images.</p><p><strong>Splits:</strong> The first version of MS COCO dataset was released in 2014. It contains 164K images split into training (83K), validation (41K) and test (41K) sets. In 2015 additional test set of 81K images was released, including all the previous test images and 40K new images.</p><p>Based on community feedback, in 2017 the training/validation split was changed from 83K/41K to 118K/5K. The new split uses the same images and annotations. The 2017 test set is a subset of 41K images of the 2015 test set. Additionally, the 2017 release contains a new unannotated dataset of 123K images.</p><p><strong>Annotations:</strong> The dataset has annotations for</p><ul><li>object detection: bounding boxes and per-instance segmentation masks with 80 object categories,</li><li>captioning: natural language descriptions of the images (see MS COCO Captions),</li><li>keypoints detection: containing more than 200,000 images and 250,000 person instances labeled with keypoints (17 possible keypoints, such as left eye, nose, right hip, right ankle),</li><li>stuff image segmentation – per-pixel segmentation masks with 91 stuff categories, such as grass, wall, sky (see MS COCO Stuff),</li><li>panoptic: full scene segmentation, with 80 thing categories (such as person, bicycle, elephant) and a subset of 91 stuff categories (grass, sky, road),</li><li>dense pose: more than 39,000 images and 56,000 person instances labeled with DensePose annotations – each labeled person is annotated with an instance id and a mapping between image pixels that belong to that person body and a template 3D model. The annotations are publicly available only for training and validation images.</li></ul><blockquote><p>检测，分割，文本图像生成</p></blockquote><h2 id="cityscapes" tabindex="-1"><a class="header-anchor" href="#cityscapes" aria-hidden="true">#</a> Cityscapes</h2>',7),_={href:"https://www.cityscapes-dataset.com/dataset-overview/",target:"_blank",rel:"noopener noreferrer"},k=e("p",null,"专注于对城市街景的语义理解。它为 30 个类提供语义、实例和密集像素注释，这些类分为 8 个类别（平面、人类、车辆、结构、对象、自然、天空和虚空）。该数据集由大约 5000 张精细注释图像和 20000 张粗略注释图像组成。在几个月、白天和良好的天气条件下，在 50 个城市捕获了数据。它最初被录制为视频，因此手动选择帧以具有以下功能：大量动态对象、不同的场景布局和不同的背景。",-1),w=e("blockquote",null,[e("p",null,"分割")],-1),v=e("h2",{id:"places",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#places","aria-hidden":"true"},"#"),t(" Places")],-1),y={href:"http://places.csail.mit.edu/browser.html",target:"_blank",rel:"noopener noreferrer"},C=e("p",null,[t("The "),e("strong",null,"Places"),t(" dataset is proposed for scene recognition and contains more than 2.5 million images covering more than 205 scene categories with more than 5,000 images per category.")],-1),x=e("blockquote",null,[e("p",null,"分类，补全")],-1),q=e("h2",{id:"ade20k",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ade20k","aria-hidden":"true"},"#"),t(" ADE20K")],-1),D={href:"https://groups.csail.mit.edu/vision/datasets/ADE20K/",target:"_blank",rel:"noopener noreferrer"},K=e("p",null,[e("strong",null,"ADE20K"),t("语义分割数据集包含超过20K个以场景为中心的图像，这些图像使用像素级对象和对象部件标签进行了详尽的注释。总共有150个语义类别，包括天空，道路，草和离散对象，如人，汽车，床。")],-1),O=e("blockquote",null,[e("p",null,"分割：语义分割、实例分割、全景分割（增加对背景的分割和检测）"),e("p",null,"semantic segmentation、instance segmentation、panopticsegmentation")],-1),T=e("h2",{id:"nyuv2-nyu-depth-v2",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#nyuv2-nyu-depth-v2","aria-hidden":"true"},"#"),t(" NYUv2 （NYU-Depth V2）")],-1),N={href:"https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html",target:"_blank",rel:"noopener noreferrer"},B=r('<p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/nyu_depth_v2_web.jpg" alt="img"></p><blockquote><p>RGB camera (left), preprocessed depth (center) and a set of labels (right) for the image.</p></blockquote><p>The <strong>NYU-Depth V2</strong> data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect. It features:</p><ul><li>1449 densely labeled pairs of aligned RGB and depth images</li><li>464 new scenes taken from 3 cities</li><li>407,024 new unlabeled frames</li><li>Each object is labeled with a class and an instance number. The dataset has several components:</li><li>Labeled: A subset of the video data accompanied by dense multi-class labels. This data has also been preprocessed to fill in missing depth labels.</li><li>Raw: The raw RGB, depth and accelerometer data as provided by the Kinect.</li><li>Toolbox: Useful functions for manipulating the data and labels.</li></ul><blockquote><p>分割，深度估计，</p><p>3D Semantic Scene Completion：目标是从未完成的 3D 输入（例如点云、深度图）和可选的 RGB 图像推断密集的 3D 体素化语义场景。</p></blockquote><h2 id="bdd100k" tabindex="-1"><a class="header-anchor" href="#bdd100k" aria-hidden="true">#</a> BDD100K</h2><blockquote><p>https://www.bdd100k.com/</p></blockquote><p><img src="https://raw.githubusercontent.com/Overmind7/images/main/img/bdd.gif" alt="img"></p><p>Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with <strong>100K videos and 10 tasks</strong> to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue. More detail is at the dataset home page.</p><blockquote><p>多目标追踪、分割、检测</p><p>车道检测、可驾驶区域检测</p></blockquote><h2 id="crowdhuman" tabindex="-1"><a class="header-anchor" href="#crowdhuman" aria-hidden="true">#</a> CrowdHuman</h2><blockquote><p>http://www.crowdhuman.org/</p></blockquote><p><strong>CrowdHuman</strong> is a large and rich-annotated human detection dataset, which contains 15,000, 4,370 and 5,000 images collected from the Internet for training, validation and testing respectively. The number is more than 10× boosted compared with previous challenging pedestrian detection dataset like CityPersons. The total number of persons is also noticeably larger than the others with ∼340k person and ∼99k ignore region annotations in the CrowdHuman training subset.</p><blockquote><p>很多人的数据集、目标检测</p></blockquote><h2 id="inaturalist" tabindex="-1"><a class="header-anchor" href="#inaturalist" aria-hidden="true">#</a> INaturalist</h2><blockquote><p>https://github.com/visipedia/inat_comp/tree/master/2017</p></blockquote><p>The iNaturalist 2017 dataset (iNat) contains 675,170 training and validation images from 5,089 natural fine-grained categories. Those categories belong to 13 super-categories including Plantae (Plant), Insecta (Insect), Aves (Bird), Mammalia (Mammal), and so on. The iNat dataset is highly imbalanced with dramatically different number of images per category. For example, the largest super-category “Plantae (Plant)” has 196,613 images from 2,101 categories; whereas the smallest super-category “Protozoa” only has 381 images from 4 categories.</p><blockquote><p>特点：各个类的数量不平衡</p><p>分类</p></blockquote>',18);function j(M,I){const s=i("router-link"),n=i("ExternalLinkIcon");return c(),d("div",null,[e("nav",p,[e("ul",null,[e("li",null,[a(s,{to:"#coco-microsoft-common-objects-in-context"},{default:o(()=>[t("COCO (Microsoft Common Objects in Context)")]),_:1})]),e("li",null,[a(s,{to:"#cityscapes"},{default:o(()=>[t("Cityscapes")]),_:1})]),e("li",null,[a(s,{to:"#places"},{default:o(()=>[t("Places")]),_:1})]),e("li",null,[a(s,{to:"#ade20k"},{default:o(()=>[t("ADE20K")]),_:1})]),e("li",null,[a(s,{to:"#nyuv2-nyu-depth-v2"},{default:o(()=>[t("NYUv2 （NYU-Depth V2）")]),_:1})]),e("li",null,[a(s,{to:"#bdd100k"},{default:o(()=>[t("BDD100K")]),_:1})]),e("li",null,[a(s,{to:"#crowdhuman"},{default:o(()=>[t("CrowdHuman")]),_:1})]),e("li",null,[a(s,{to:"#inaturalist"},{default:o(()=>[t("INaturalist")]),_:1})])])]),u,m,g,e("p",null,[e("a",b,[t("COCO - Common Objects in Context (cocodataset.org)"),a(n)])]),f,e("blockquote",null,[e("p",null,[e("a",_,[t("Dataset Overview – Cityscapes Dataset (cityscapes-dataset.com)"),a(n)])])]),k,w,v,e("blockquote",null,[e("p",null,[e("a",y,[t("places.csail.mit.edu/browser.html"),a(n)])])]),C,x,q,e("blockquote",null,[e("p",null,[e("a",D,[t("ADE20K dataset (mit.edu)"),a(n)])])]),K,O,T,e("blockquote",null,[e("p",null,[e("a",N,[t("NYU Depth V2 « Nathan Silberman"),a(n)])])]),B])}const S=l(h,[["render",j],["__file","数据集.html.vue"]]);export{S as default};
