# ConceptGraphs

Open-Vocabulary 3D Scene Graphs for Perception and Planning

ConceptGraphs: 用于感知和规划的开放词汇3D场景图

## 摘要：
为了让机器人执行各种任务，它们需要一个语义丰富、紧凑且高效的3D世界表示，以便于任务驱动的感知和规划。最近的方法尝试利用大型视觉-语言模型的特征来在3D表示中编码语义。然而，这些方法往往产生每点特征向量的地图，这在更大的环境中扩展性不好，也不包含环境中实体之间有用的语义空间关系，这对于后续规划是有益的。在这项工作中，我们提出了ConceptGraphs，这是一种用于3D场景的开放词汇图结构表示。

ConceptGraphs通过利用2D基础模型并将其输出与3D融合来构建。得到的表示对新语义类别具有泛化性，无需收集大型3D数据集或微调模型。我们通过一系列下游规划任务展示了这种表示的实用性，这些任务通过抽象（语言）提示指定，并需要对空间和语义概念进行复杂推理。为了探索我们实验和结果的全部范围，我们鼓励读者访问我们的项目网页。

![splash-final.png](https://raw.githubusercontent.com/Overmind7/images/main/img/splash-final.png)

## I. 引言

场景表示是促进各种任务（包括移动性和操作性）下游规划的关键设计选择之一。机器人需要在线从机载传感器构建这些表示，因为它们在环境中导航。为了有效执行复杂任务，这些表示应该是：随着场景体积和机器人操作时间的增加，可扩展且易于维护；开放词汇，不仅限于在推理时对一组在训练时预定义的概念进行推断，而且能够在推理时处理新对象和概念；并且具有灵活的细节层次，以便规划从需要密集几何信息用于移动性和操作性的任务，到需要抽象语义信息和对象级可供性信息用于任务规划的任务。我们提出了ConceptGraphs，这是一种用于机器人感知和规划的3D场景表示方法，满足上述所有要求。

### A. 相关工作
在3D中进行封闭词汇的语义映射。早期的工作通过如同时定位与地图构建（SLAM）[1]-[5]或离线方法如从运动中恢复结构（SfM）[6]、[7]等在线算法重建3D地图。除了重建3D几何形状，最近的工作还使用基于深度学习的对象检测和分割模型来重建具有密集语义映射的3D场景表示[8]-[11]或对象级分解[12]-[15]。虽然这些方法在将语义信息映射到3D方面取得了令人印象深刻的结果，但它们是封闭词汇的，其适用性限于训练数据集中注释的对象类别。使用基础模型的3D场景表示。最近有重要的努力[16]-[30]专注于通过利用基础模型——大型、功能强大的模型，它们捕获了多样化的概念并完成了广泛的任务[31]-[35]——来构建3D表示。这些模型在解决2D视觉中的开放词汇挑战方面表现出色。然而，它们需要“互联网规模”的训练数据，而且目前还没有可比较大小的3D数据集。因此，最近的工作尝试将图像和语言基础模型产生的2D表示与3D世界联系起来，并在包括语言引导的对象定位[17]、[18]、[24]、[26]、[36]，3D推理[37]、[38]，机器人操作[39]、[40]和导航[41]、[42]在内的开放词汇任务上展示了令人印象深刻的结果。这些方法将图像中的密集每像素特征投影到3D，以构建如点云[17]-[21]或隐式神经表示[16]、[22]-[30]等显式表示。

然而，这些方法有两个主要限制。首先，给每个点分配一个语义特征向量是高度冗余的，并且比必要的消耗更多的内存，极大地限制了在大型场景中的可扩展性。其次，这些密集表示不容易分解——这种缺乏结构使得它们对地图的动态更新（对机器人至关重要）不太友好。3D场景图。3D场景图（3DSGs）通过紧凑高效地描述场景来解决第二个限制，节点代表对象，边编码对象间关系[43]-[47]。这些方法使得能够构建实时系统，这些系统可以动态构建层次化的3D场景表示[48]-[50]，并且最近还展示了各种机器人规划任务如何从3DSGs的效率和紧凑性中受益[51]、[52]。然而，现有的构建3D场景图的工作仅限于封闭词汇设置，限制了它们在一组任务中的适用性。

### B. 我们的贡献概述
在这项工作中，我们克服了所有前述的限制，并提出了ConceptGraphs，这是一种用于机器人感知和规划的开放词汇和以对象为中心的3D表示。在ConceptGraphs中，每个对象都作为具有几何和语义特征的节点来表示，对象间的关系在图的边中编码。ConceptGraphs的核心是一种以对象为中心的3D映射技术，它整合了传统3D映射系统的几何线索和视觉与语言基础模型的语义线索[31]、[33]、[34]、[53]-[55]。对象通过利用大型语言模型（LLMs）[32]和大型视觉-语言模型（LVLMs）[55]来分配语言标签，这些模型提供了丰富的语义描述，并允许自由形式的语言查询，同时使用现成的模型（无需训练/微调）。

场景图结构使我们能够高效地表示大型场景，占用的内存很少，并便于高效任务规划。在实验中，我们展示了ConceptGraphs能够发现、映射并描述场景中的大量对象。此外，我们在多个机器人平台上进行了真实世界的试验，涵盖了广泛的下游任务，包括操作、导航、定位和地图更新。总之，我们的主要贡献是：

- 我们提出了一种新颖的以对象为中心的映射系统，它整合了传统3D映射系统的几何线索和2D基础模型的语义线索。
- 我们构建了开放词汇的3D场景图；用于感知和规划的高效且结构化的语义抽象。
- 我们在真实世界的轮式和腿部机器人平台上实现了ConceptGraphs，并展示了多种用于复杂/抽象语言查询的下游感知和规划能力。

如果您需要其他部分的翻译或有其他问题，请告诉我。





## II. 方法

ConceptGraphs构建了一个紧凑、语义丰富的3D环境表示。给定一组姿势RGB-D帧，我们运行一个类别无关的分割模型来获取候选对象，使用几何和语义相似性度量将它们关联到多个视图中，并在3D场景图中实例化节点。然后我们使用一个大型视觉-语言模型（LVLM）为每个节点生成标题，一个大型语言模型（LLM）来推断相邻节点之间的关系，这在场景图中产生边。这个结果场景图是开放词汇的，封装了对象属性，并且可以用于多种下游任务，包括分割、对象定位、导航、操作、定位和重映射。该方法在图2中进行了说明。





![img](https://raw.githubusercontent.com/Overmind7/images/main/img/pipeline-20240903102939890.png)

> 图2：ConceptGraphs从一系列带位姿的RGB-D图像构建开放词汇的3D场景图。我们使用通用实例分割模型从RGB图像中分割区域，提取每个区域的语义特征向量，并将它们投影到3D点云中。这些区域从多个视图递增地关联和融合，形成一组3D对象和相关的视图（和语言）描述符。然后，我们使用大型视觉和语言模型为每个映射的3D对象生成标题，并推导出对象间的关系，生成连接这组对象并形成图的边。结果3D场景图提供了对场景的结构化和全面的了解，并且可以很容易地被翻译成文本描述，这对于基于LLM的任务规划非常有用。





### A. 基于对象的3D映射

---------------------

**对象中心的3D表示**

给定一系列RGB-D观测值 $I = \{I_1, I_2, ..., I_t\}$，ConceptGraphs构建一个地图，一个3D场景图 $M_t = \langle O_t, E_t \rangle$，

- 其中 $O_t = \{o_j\}^J_{j=1}$ 和 $E_t = \{e_k\}^K_{k=1}$ 分别代表对象和边的集合。

- 每个对象 $o_j$ 由一个3D点云 $p_{oj}$ 和一个语义特征向量 $f_{oj}$ 表征。

这个地图是增量构建的，通过将每个传入的帧 $I_t = \langle I_{rgb_t}, I_{depth_t}, \theta_t \rangle$（彩色图像、深度图像、姿态）并入现有的对象集合 $O_{t-1}$ ，通过添加到现有对象或实例化新对象。

--------------------

**类别无关的2D分割**

在处理帧 $I_t$ 时

- 一个类别无关的分割模型 $\text{Seg}(\cdot)$ 被用来获得一组掩模 $\{m_{t,i}\}^M_{i=1} = \text{Seg}(I_{rgb_t})$ ，对应于候选对象。

- 然后，将每个提取的掩模 $m_{t,i}$ 传递给一个视觉特征提取器（例如CLIP[31]、DINO[53]）以获得一个视觉描述符 $f_{t,i} = \text{Embed}(I_{rgb_t}, m_{t,i})$。

- 每个遮罩区域被投影到3D，使用DBSCAN聚类进行去噪，并转换到地图帧。

这导致了一个点云 $p_{t,i}$ 及其对应的单位归一化语义特征向量 $f_{t,i}$。

-----------------

**对象关联**

对于每个新检测到的对象 $\langle p_{t,i}, f_{t,i} \rangle$，我们计算它与地图中所有对象 $o_{t-1,j} = \langle p_{oj}, f_{oj} \rangle$的**语义**和**几何相似性**，这些对象在任何部分几何重叠。

- 几何相似性
    - $\phi_{\text{geo}}(i, j) = \text{nnratio}(p_{t,i}, p_{oj})$ 
    - 是点云 $p_{t,i}$ 中在点云 $p_{oj}$ 内有最近邻的点的比例，距离阈值为 $\delta_{nn}$。

- 语义相似性
    - $\phi_{\text{sem}}(i, j) = \frac{f_{t,i}^T f_{oj}}{2} + \frac{1}{2}$ 
    - 是相应视觉描述符之间的归一化余弦距离。

整体相似性度量$\phi(i, j)$是两者之和：
$$
\phi(i, j) = \phi_{\text{sem}}(i, j) + \phi_{\text{geo}}(i, j)
$$
我们通过贪婪分配策略进行对象关联，其中每个新检测与具有最高相似性分数的现有对象匹配。如果没有找到相似性高于 $\delta_{sim}$ 的匹配，则初始化一个新对象。

-------------------------

**对象融合**

如果检测 $o_{t-1,j}$ 与映射对象 $o_j$ 关联，则我们将检测与地图融合。

- 这是通过更新对象语义特征 $f_{oj} = \frac{n_{oj} f_{oj} + f_{t,i}}{n_{oj} + 1}$ 来实现的，
    - 其中$n_{oj}$是到目前为止与 $o_j$ 关联的检测数量
- 并更新点云为 $p_{t,i} \cup p_{oj}$
- 然后进行下采样以去除冗余点。

----------------------------

**节点描述**

Node Captioning

一旦整个图像序列被处理，一个视觉-语言模型，记为 LVLM(·)，被用来生成对象描述。

- 对于每个对象，将来自最佳10个视图的关联图像裁剪传递给语言模型，
    - 提示为“描述图像中的中心对象”
    - 生成每个检测到的对象 $o_j$ 的一组初始粗略描述 $\hat{j} = \{\hat{j}_1, \hat{cj}_2, ..., \hat{cj}_{10}\}$。
- 然后通过将 $\hat{cj}$ 传递给另一个语言模型 LLM(·)，使用提示指令将每组`描述`总结为一个连贯且准确的最终描述 $c_j$。



### B. 场景图生成
给定从上一步获得的一组3D对象 $O_T$，我们估计它们的空间关系，即边 $E_T$，以完成3D场景图。

- 我们首先通过计算每对对象节点之间的3D边界框 IoU 来估计对象节点之间的潜在连接性，获得一个相似性矩阵（即，一个密集图），我们通过估计最小生成树（MST）来修剪它，从而得到对象之间潜在边的精炼集合。
- 为了进一步确定语义关系，对于MST中的每条边，我们将关于对象对的信息，包括对象标题和3D位置，输入到一个语言模型LLM中。提示指示模型描述对象之间的可能空间关系，例如“a在b上”或“b在a中”，以及背后的推理。该模型输出一个关系标签和详细说明推理的解释。

使用LLM允许我们将上述定义的名义边类型扩展到语言模型可以解释的其他输出关系，例如“一个背包可能存放在衣柜中”和“纸张可以回收到垃圾桶中”。这导致了一个开放词汇的3D场景图 $M_T = (O_T, E_T)$，这是一个紧凑且高效的表示，用于下游任务。

### C. 通过LLMs进行机器人任务规划
为了使用户能够执行自然语言查询描述的任务，我们将场景图$M_T$与LLM接口。对于$O_T$中的每个对象，我们构建包含其3D位置（边界框）和节点标题的JSON结构化文本描述。给定一个文本查询，我们要求LLM识别场景中与查询最相关的对象。然后，我们将这个对象的3D姿态传递给适当的下游任务（例如，抓取、导航）流程。这种将ConceptGraphs与LLM集成的方法易于实现，并允许通过为机器人提供周围对象的语义属性，从而执行广泛的开放词汇任务。

### D. 实施细节
ConceptGraphs的模块化使得可以使用任何适当的开放/封闭词汇分割模型、LLM或LVLM。我们的实验使用SegmentAnything (SAM) [33]作为分割模型 $\text{Seg}(\cdot)$，使用CLIP图像编码器[31]作为特征提取器 $\text{Embed}(\cdot)$。我们使用LLaVA [55]作为视觉-语言模型LVLM，使用GPT-4 [32]（gpt-4-0613）作为我们的LLM。点云下采样和最近邻阈值$\delta_{nn}$的体素大小均为2.5cm。我们将关联阈值 $\delta_{sim}$ 设为1.1。我们还开发了我们系统的一个变体，ConceptGraphsDetector (CG-D)，我们使用图像标记模型（RAM [54]）列出图像中出现的对象类别，以及一个开放词汇2D检测器（Grounding DINO [34]）来获取对象边界框。在这个变体中，我们需要单独处理检测到的背景对象（墙、天花板、地板），无论它们的相似性得分如何，都将它们合并在一起。



## III. 实验

### A. 场景图构建
我们首先评估了ConceptGraphs系统输出的3D场景图的准确性。对于Replica数据集中的每个场景，我们报告了CG和检测器变体CG-D的场景图准确性指标。由于我们系统的开放词汇特性，自动化评估场景图中节点和边的质量是具有挑战性的。我们通过在亚马逊机械土耳其（AMT）上聘请人类评估员来评估场景图。对于每个节点，我们计算准确度，即至少有2/3的人类评估员认为节点标题正确的节点比例。我们还报告了每个变体通过询问评估员他们是否认为每个节点是有效对象来检索的有效对象数量。CG和CG-D都在每个场景中识别出许多有效对象，并且只产生少量（0-5）的重复检测。节点标签大约70%的时间是准确的；大多数错误是由所采用的LVLM（LLaVA）造成的。边（空间关系）的标签具有很高的准确性（平均90%）。

### B. 3D语义分割
ConceptGraphs专注于构建用于场景理解和规划的开放词汇3D场景图。为了完整性，在本节中，我们还使用开放词汇3D语义分割任务来评估所获得的3D地图的质量。为了生成语义分割，给定一组类名，我们计算每个对象节点的融合语义特征与CLIP文本嵌入的相似度，短语为“an image of {class}”。然后，将与每个对象关联的点分配给相似度最高的类别，从而得到具有密集类标签的点云。在表II中，我们报告了在Replica数据集上的语义分割结果，并遵循了ConceptFusion使用的评估协议。我们还提供了一个额外的基线，ConceptFusion+SAM，通过用更高性能的SAM模型替换ConceptFusion中使用的Mask2Former。如表II所示，所提出的ConceptGraphs的表现与ConceptFusion相当或更好，后者的内存占用要大得多。

### C. 基于文本查询的对象检索
我们评估了ConceptGraphs处理复杂语义查询的能力，重点关注三种关键类型：

- 描述性：例如，一个盆栽植物。
- 可供性：例如，用于临时固定破损拉链的东西。
- 否定性：例如，除了苏打水以外的饮料。

我们在Replica数据集和一个真实世界的REAL实验室扫描上进行评估，其中我们设置了包括衣服、工具和玩具在内的许多项目。对于Replica，人类评估员在AMT上注释了SAM掩模提议的标题，这些标题既作为真实标签也作为描述性查询。我们为Replica中的每种场景类型（办公室和房间）创建了5个可供性和否定性查询，并为实验室扫描创建了10个每种类型的查询，确保每个查询至少对应一个相关对象。我们手动选择相关对象作为每个查询的真实标签。我们使用两种对象检索策略：基于CLIP的和基于LLM的。CLIP选择与查询嵌入最相似的对象，而LLM则通过场景图节点来识别具有最相关标题的对象。表III显示，CLIP在描述性查询方面表现出色，但在处理复杂的可供性和否定性查询方面却很挣扎。例如，CLIP错误地为破损拉链查询检索了一个背包，而LLM正确地识别了一卷胶带。LLM在所有方面都表现良好，但受到节点标题准确性的限制，如第III-A节所讨论的。由于实验室有更多种类的对象可供选择，LLM在复杂查询中更可靠地找到兼容对象。

### D. 复杂视觉-语言查询
为了评估ConceptGraphs在真实环境中的表现，我们在REAL实验室场景中进行了导航实验，使用的是Clearpath Jackal UGV。该机器人配备了VLP-16 LiDAR和前置Realsense D435i相机。Jackal需要响应抽象用户查询并导航至最相关对象（见图1）。通过使用LVLM[55]为当前相机图像添加描述到文本提示中，机器人还可以回答视觉查询。例如，当展示迈克尔·乔丹的图片并提示“这个家伙会玩的东西”时，机器人找到了一个篮球。

### E. 对象搜索和可通行性估计
在本节中，我们展示了ConceptGraphs表示与LLM交互如何使移动机器人能够访问日常对象的广泛知识库。具体来说，我们提示LLM从ConceptGraphs标题中推断两个额外的对象属性：i)给定对象通常被发现的位置，以及ii)对象是否可以安全地被Jackal机器人推动或通过。我们围绕LLM预测设计了两个任务。对象搜索：机器人接收到一个抽象用户查询并必须导航至ConceptGraphs地图中最相关对象。使用LVLM[55]，机器人随后检查对象是否在预期位置。如果不是，它将查询LLM以找到给定标题的其他对象在场景中的可能位置。在我们的提示中，我们鼓励LLM考虑典型的容器或存储位置。我们在图3中展示了两个这样的查询，目标对象被移动了。可通行性估计：如图4所示，我们设计了一个真实世界场景，机器人发现自己被对象包围。在这种情况下，机器人必须推动多个对象并创建一条通往目标状态的路径。虽然可以通过经验学习可通行性[62]，但我们展示了将LLM知识定位在3D地图中可以为机器人代理提供类似能力。

### F. 开放词汇拿起放置
为了说明ConceptGraphs如何作为开放词汇移动操作的感知骨干，我们与波士顿动力Spot Arm机器人一起进行了一系列的实验。使用机载RGBD相机和场景的ConceptGraphs表示，Spot机器人响应“cuddly quacker”查询，抓取一个鸭子毛绒玩具并将其放置在附近的盒子中（见图1）。在补充视频中，Spot在被提示“something healthy to eat”时，完成了与芒果相似的抓取动作。

### G. 定位和地图更新
ConceptGraphs也可用于基于对象的定位和地图更新。我们在AI2Thor[63]、[64]仿真环境中展示了这一点，其中移动机器人使用粒子滤波器在预先构建的ConceptGraphs环境地图中定位。在粒子滤波的观察更新步骤中，机器人的检测结果根据假设的姿态与地图中的对象进行匹配，类似于第II-A节中描述的方式。匹配结果被聚合成一个单一的观察分数，用于对姿态假设进行加权。在此过程中，先前观察到的对象如果未被机器人观测到则被移除，也可以添加新对象。我们在补充视频材料中提供了这种定位和地图更新方法的演示。

### H. 限制
尽管ConceptGraphs表现出色，但仍有一些失败模式需要在未来的工作中解决。首先，由于当前LVLMs（如LLaVA[55]）的限制，节点标题会产生错误。其次，我们的3D场景图有时会错过小或薄的物体，并产生重复检测。这影响了下游规划，特别是当错误的检测对规划成功至关重要时。此外，我们系统的计算和经济成本包括多个LVLM（LLaVA[55]）和一次或多次专有LLM推断，这可能是相当大的。



## IV. 同期工作

我们简要回顾了最近和未发表的预印本，这些文献探讨了与开放词汇基于对象的3D场景分解相关主题。与我们同时，[65]、[66]探索了3D场景的开放词汇基于对象的分解。其中[65]假设有一个预先构建的场景点云地图，而[66]则是边构建地图边进行。两种方法都将CLIP描述符关联到重建中，其性能与我们系统的CLIP变体相当，后者在涉及复杂功能和否定的查询中表现挣扎，如表III所示。OGSV [67]与我们的设置更接近，它从RGB-D图像中构建开放词汇的3D场景图。然而，[67]依赖于（封闭集的）图神经网络来预测对象关系；而ConceptGraphs则依赖于现代LLMs的能力，消除了训练对象关系模型的需要。





## V. 结论

在本文中，我们介绍了ConceptGraphs，这是一种新颖的开放词汇对象中心3D场景表示方法，它解决了现有密集和隐式表示方法中的关键限制。通过有效地整合基础的2D模型，ConceptGraphs显著缓解了内存限制问题，提供了对象间的关系信息，并允许对场景进行动态更新——这是当前方法中普遍存在的三个挑战。实验证据强调了ConceptGraphs的鲁棒性和可扩展性，突显了其在各种真实世界任务（包括操作和导航）中相对于现有基线的优越性。我们框架的多功能性还适应了广泛的下游应用，从而为机器人感知和规划的创新开辟了新的途径。未来的工作可能会深入研究将时间动态整合到模型中，并在结构更少、更具挑战性的环境中评估其性能。
