# Online3D

> Memory-based Adapters for Online 3D Scene Perception
>
> Xiuwei Xu
>
> CVPR, 2024



![overview](https://raw.githubusercontent.com/Overmind7/images/main/img/over-arch.png)

## Abstract

在本文中，我们提出了一个新的在线3D场景感知框架。传统的3D场景感知方法是离线的，即以已经重建的3D场景几何体为输入，这并不适用于机器人应用中，其中输入数据是流式的RGB-D视频，而不是预先收集的RGB-D视频中重建的完整3D场景。为了处理在线3D场景感知任务，其中数据收集和感知应该同时进行，模型应该能够逐帧处理3D场景并利用时间信息。为此，我们为3D场景感知模型的主干提出了一个基于适配器的即插即用模块，构建记忆以缓存和聚合提取的RGB-D特征，赋予离线模型时间学习能力。具体来说，我们提出了一种排队记忆机制，用于缓存支持的点云和图像特征。然后我们设计了聚合模块，它们直接在记忆上操作，并将时间信息传递给当前帧。我们进一步提出了3D到2D适配器，以增强具有强大全局上下文的图像特征。我们的适配器可以轻松地插入到不同任务的主流离线架构中，并显著提高它们在在线任务上的性能。在ScanNet和SceneNN数据集上的广泛实验表明，

我们的方法在三个3D场景感知任务上与最先进的在线方法相比，通过简单地微调现有的离线模型，无需任何模型和任务特定设计，就实现了领先的性能。



## 1. Intro

3D场景感知旨在将3D场景解析成语义或对象级实体，主要包括语义分割、对象检测和实例分割，这是机器人或AR/VR应用的基础能力。自从PointNet[32]提出了第一个直接处理点云的模型以来，近年来通过精确和高效的架构设计，在3D场景感知[6, 38, 40, 42, 43]方面取得了巨大的进步。然而，传统的3D场景感知方法是离线的，即它们以预先收集的RGB-D视频中重建的3D场景几何体为输入，而不包含时间信息。而在大多数机器人应用，如导航[4, 47]和操纵[25]中，代理通常在未知环境中初始化，输入数据是流式的RGB-D视频，场景感知应该与数据收集同步进行，以指导代理如何探索。因此，需要具有时间建模能力的在线3D场景感知模型，该模型接收流式的RGB-D视频并持续输出当前观察到的3D场景的感知。也有一些为特定架构和任务设计在线3D场景感知方法[16, 22, 24, 26, 46]。由于这些方法只关注单模态的时间聚合，它们无法充分利用图像和点云特征之间的时间关系，因此它们的表现并不令人满意。在本文中，我们提出了一个新的通用框架，用于在线3D场景感知。与以往基于特定架构和任务设计在线感知方法并从头开始训练模型的工作不同，我们通过简单地插入即插即用模块和微调，将现有的离线3D感知模型转换为在线模型。受到适配器[5, 29]的启发，这些适配器通过额外的参数调整使图像主干适应下游任务，我们提出了基于记忆的适配器，通过重用以前帧中提取的特征，赋予3D感知模型主干时间建模能力。具体来说，我们提出了一种排队记忆机制，用于缓存当前时间的RGB-D帧的支撑点云和图像特征。基于记忆的结构，我们设计了聚合模块，它们直接在记忆上操作，并将时间信息传递给当前帧。由于图像特征的全局上下文是有限的，我们进一步提出了3D到2D适配器，通过3D记忆投影和2D稀疏聚合增强图像特征。通过这种方式，我们可以利用现有的主流3D场景感知模型库来获取一系列在线模型，只需简单插入和微调。我们在ScanNet[7]和SceneNN[15]数据集上进行了广泛的实验。我们的方法在所有任务和数据集上都实现了领先的性能，无需任何额外的损失函数和特殊的预测融合策略。总结来说，我们的贡献包括：

- 我们提出了一个新的在线3D场景感知框架，通过适配器将现有的离线模型扩展到在线模型，无需模型和任务特定设计。
- 我们提出了通用的基于记忆的适配器，用于图像和点云主干，它们缓存和聚合提取的特征，以模拟帧之间的时间关系。
- 配备了我们的适配器，离线模型能够在三个任务上实现与最先进的在线模型相比的领先性能。





## 2. 相关工作

**3D场景感知**：3D场景感知在计算机视觉中被广泛研究，可以分为三个主流任务：语义分割[6, 11, 32, 33]、对象检测[13, 34, 38, 43]和实例分割[17, 18, 40, 42, 45]。由于我们这项工作主要关注3D场景的特征提取，所以我们主要讨论3D场景感知网络的主干。由于点云数据的无序属性，将点体素化并在3D网格上应用卷积是一种自然的解决方案[3, 31]。然而，随着体素分辨率的增加，计算成本和内存需求都呈立方级增长，这是低效的。PointNet[32]是直接从原始点云中提取特征表示的开创性工作。PointNet++基于PointNet提出了集合抽象和特征传播操作，这有助于学习更详细的局部几何信息。由于PointNet++中的最远点采样操作耗时较长，PV-CNN[23]将点云转换为低分辨率体素并应用3D卷积以高效聚合局部特征。另一种提取高质量3D特征的方法是稀疏卷积[9, 10]，它将点云体素化但只在非空体素上应用3D卷积。为了进一步提高稀疏CNN的效率，引入了子流形稀疏卷积[6, 11]，它仅在卷积核中心滑过活动站点时进行卷积，并在整个网络中保持相同级别的稀疏性。然而，这些方法都是为离线3D场景感知设计的，无法实时处理流式的RGB-D视频。

**流数据分析**：由于在线3D场景感知模型的输入是流式的RGB-D视频，我们回顾了图像和点云领域的流数据分析方法。在2D视觉中，许多工作[2, 8, 19]将因果卷积[28]扩展到流式视频，其中设计了一个流缓冲区来缓存以前的帧，并对空间时间信息应用3D因果卷积进行单向聚合。TSM[20]采用了更高效的移位机制，将以前图像特征的一部分通道移至下一帧。然后可以通过2D卷积高效聚合空间时间信息。我们的图像适配器也利用通道移位进行有效的时间建模。不同地，TSM是从零开始训练的，网络可以学习如何根据移位比例模拟时间信息。而我们则在即插即用的适配器中重新组织通道并采用通道移位，赋予图像主干时间建模能力。然而，由于2D流视频包含的信息对于现实世界应用（如机器人导航[4, 47]和操纵[25]）帮助较少，因此越来越多地关注流式RGB-D视频分析。一个自然解决方案是首先处理2D图像，然后将预测投影到3D点云上，然后通过融合步骤合并不同帧的预测[24, 26]。Fusion-aware 3D-Conv[46]和SVCNN[16]在3D空间中维护以前帧的信息，并进行基于点的卷积以融合3D特征进行语义分割。INS-CONV[22]将稀疏卷积扩展到增量CNN，以高效提取全局3D特征，用于语义和实例分割。不同地，我们的方法通过图像和点云基于记忆的适配器，赋予离线模型在线感知能力，充分利用多模态时间关系。

## 3. 方法

在本节中，我们首先介绍在线3D场景感知的定义，并解释我们基于记忆的适配器的动机。然后我们描述如何构建记忆并通过适配器分别对点云和图像的主干特征进行细化。

### 3.1 在线3D场景感知

设 $X_t = \{x_1, x_2, ..., x_t\}$ 是一个带姿势的RGB-D流视频，这意味着视频是随着传感器的移动而收集的，而不是预先收集的视频。我们有：

\[x_t = (I_t, P_t, M_t), I_t \in \mathbb{R}^{H \times W \times 3}, P_t \in \mathbb{R}^{N \times 3}, M_t \in \mathbb{R}^{3 \times 4}\]
其中 $I_t$ 和 $P_t$ 分别指一个RGB-D帧的图像和点云。 $P_t$ 是通过使用位姿参数 $M_t$ 将深度图像提升到世界坐标系中获得的，其中 $M_t$ 可以通过视觉里程计[30, 48]估计。在时间 $t$ ，在线感知模型的输入是 $X_t$ ，输出是观察到的3D场景 $S_t = \bigcup_{i=1}^{t} P_i$ 的预测，可以是边界框和语义/实例掩码。一些工作还同时执行3D重建和在线感知[22]，以获取高质量的点云或网格，但这不是必须的[4, 35, 47]。在这项工作中，我们不依赖于3D重建，直接以RGB-D流视频为输入，这是一个更普遍的设置。尽管在3D感知模型的设计上取得了很大的进步，但它们大多只关注两种场景：(1) 重建场景[1, 7]。模型 $M_{Rec}$ 是在RGB-D视频重建的完整场景的点云上训练的。(2) 单视图场景[27, 41]。模型 $M_{SV}$ 是在从RGB-D图像反投影的单视图点云上训练的。然而， $M_{Rec}$ 要求输入是一个完整的场景，在实时任务中是无法访问的。 $M_{SV}$ 能够逐帧处理RGB-D视频，但未能利用时间信息。因此，以前的3D模型并不适用于更实用的在线场景感知。

为此，我们旨在设计一个即插即用的时空学习模块，它可以插入到任何单视图感知模型 $M_{SV}$ 中，并赋予它时间建模能力。注意， $M_{SV}$ 最初是一个3D感知模型。我们可以通过早期融合图像特征到点云[39]将其扩展到RGB-D感知模型：

\[p_t = M_{SV} (P'_t),\]
\[P'_t = P_t \oplus S(MI(I_t), P_t, M_t), S(\cdot) \in \mathbb{R}^{N \times C}\]
_

_其中 $p_t$ 是时间 $t$ 输入帧的预测。 $MI$ 是在同一感知任务上预训练的图像主干，与 $M_{SV}$ 相同。 $S$ 通过 $M_t$ 将 $P_t$ 投影到图像坐标系，并采样相应的2D特征来丰富 $P_t$ 的特征。我们将 $M_{SV}$ 分为一个用于提取点云特征的主干 $MP$ 和一个特定任务的头部 $MH$ 。这项工作的目的是构建一个图像记忆基适配器 $AI(MI(I_t))$ 和一个点云记忆基适配器 $AP(MP(P'_t))$ ，以存储和重用提取的主干特征进行时间建模：
$$
MI(I_t), mI_t = AI(MI(I_t), mI_{t-1}, mP_{t-1}),
MP(P'_t), mP_t = AP(MP(P'_t), mP_{t-1})
$$

其中记忆基适配器 $A$ 使用当前特征更新记忆 $m_t$ ，并通过重用记忆来细化当前特征。我们充分利用了模态间和模态内的关系： $M_{SV}$ 将图像特征融合到点云中， $AP$ 重用以前的点云特征来细化当前的点云特征，而 $AI$ 重用两种模态的以前特征来细化当前图像特征。如图2所示，我们遵循相同的范式设计这两个模块，它们可以轻松嵌入到图像和点云主干中，以实现时间建模。

### 3.2 点云的时间建模

给定时间 $t$ 的 ${MP(P'_1), MP(P'_2), ..., MP(P'_t)}$ ，即从主干提取的点云特征序列，我们的目标是通过利用这个序列内的时间关系来增强当前特征 $MP(P'_t)$ 。这里我们首先构建一个记忆来有效地缓存不同时间戳的点云特征。然后我们使用即插即用的适配器将时间信息从记忆聚合到当前时间 $t$ 的特征。记忆构建：3D场景的时间信息反映在更完整的几何上。由于单个RGB-D帧可能不包含完整的大型对象或高级场景上下文，以前帧的几何信息对于当前帧的准确感知很重要。因此，我们可以在共享的3D空间中缓存提取的点云特征序列。一个简单的方法是直接将点云的特征存储在世界坐标系中。然而，这种方法在存储和计算上都是低效的：(1) 由于点云的坐标是实数值，即使RGB-D相机没有移动，存储开销也会不断增长；(2) 随着时间的推移，点的数量会非常多，因此基于点的采样和特征聚合会占用很高的计算开销。为此，我们提出将特征存储在量化坐标系统中，将点云体素化并存储在3D网格中。我们还维护一个队列来存储体素，以减少场景过大时的内存占用。具体来说，首先将 $MP(P'_t)$ 体素化到体素网格 $V_t$ 中，通过平均所有坐标落入同一网格的特征。这些体素被标记为时间戳 $t$ 。然后我们通过最大池化将 $V_t$ 合并到记忆 $mP_{t-1}$ 中：
\[mP_t = \text{maxpooling}(V_t, mP_{t-1}),\]
\[mP_t = \text{deq}(mP_t, l) \text{ if } N(mP_t) > N_{\text{max}} \]
其中最大池化指的是在每个体素网格上进行的通道-wise最大池化，它将更新特征和时间戳。 $\text{deq}(\cdot, l)$ 意味着从记忆中移除时间戳早于 $t - l + 1$ 的体素。 $N(mP_t)$ 是记忆中体素的数量。我们使用最大池化是因为它保留了随时间最具有区分性的特征，这也使得计算效率高，因为只需要 $t-1$ 和 $t$ 时刻的特征。记忆基适配器：在缓存和更新体素中的点云特征后，我们需要利用 $mP_t$ 来增强 $MP(P'_t)$ 与时间信息。为了在减少冗余计算的同时利用记忆中丰富的场景上下文，我们首先使用 $V_t$ 的坐标查询一个邻域体素集合：
\[N(V_t) = \{mP_t[x][y][z] | (x, y, z) \in s * B(V_t)\}\]
其中 $N(V_t)$ 是 $V_t$ 的查询邻域。 $mP_t[x][y][z]$ 指的是在坐标 $(x, y, z)$ 处的 $mP_t$ 中的体素。 $B$ 是 $V_t$ 的最小轴对齐包围盒， $s$ 是扩大盒子尺寸的比例因子。通过这种方式，为当前帧提供支持几何信息的时间信息被收集到这个体素集合中。然后我们将 $N(V_t)$ 转换为稀疏张量[6, 11]，然后通过3D稀疏卷积模块 $AP$ 聚合 $N(V_t)$ 中上下文信息到 $V_t$ 的位置。最后我们以适配器方式更新 $MP(P'_t)$ ：(1)我们将聚合的特征映射回 $MP(P'_t)$ 的坐标，然后通过残差连接将其添加到原始特征；(2)适配器模块 $AP$ 是零初始化的。因此，在插入适配器后，微调将从原始点开始平滑进行。

### 3.3 图像的时间建模

对于时间 $t$ 的图像特征序列 ${MI(I_1), ..., MI(I_t)}$ ，我们遵循与点云对应的相同范式来存储它，并使用记忆和适配器将时间信息聚合到当前帧 $MI(I_t)$ 。

记忆构建：不同于3D数据，不同时间戳的点云可以存储在共享的3D空间中，对于2D数据，图像特征被堆叠成流视频。处理这类数据的一个常见做法[19]是维护一个队列并执行因果卷积（沿时间维度单向）来聚合来自先前帧的信息到当前图像特征。然而，视频分析方法专注于提取到目前为止流视频中的全局信息，而在我们的情况下，我们只需要增强当前图像特征 $MI(I_t)$ 。因此，对先前帧执行因果卷积将带来大量的冗余计算。此外，在在线3D场景感知中，图像特征的最重要信息是对象从多个视角的观察。由于一个对象通常在几个相邻帧中被观察到，所以在大多数情况下，维护一个短队列就足够了。为此，我们做出了极端简化：我们只存储一帧先前的图像，并在时间 $t-1$ 和 $t$ 的帧之间利用时间关系。为了有效聚合相邻帧，我们采用通道移位来缓存时间信息。形式上，给定 $MI(I_t) \in \mathbb{R}^{H \times W \times C}$ ，我们学习一个线性变换 $R1 \in \mathbb{R}^{C \times C'}$ 将图像特征映射到另一个嵌入空间，其中前 $\frac{1}{\tau}$ 个通道包含与下一帧相关的丰富时间信息。因此，记忆可以简单地通过移位这部分通道来构建：
\[mI_t = (MI(I_t) \cdot R1)[:, :, : \frac{C'}{\tau}]\]
注意这个操作是逐帧重复的，因此 $mI_{t-1}$ 包含与当前特征 $MI(I_t)$ 相关的时间信息。记忆基适配器：在时间 $t$ ，将一部分通道移位到记忆后，我们可以用先前的记忆 $mI_{t-1}$ 填充空通道。通过这种方式，相邻两帧的时间信息被合并到单个帧中，我们可以直接采用2D卷积来聚合特征：
\[F_t = 2D-Conv(mI_{t-1} \oplus (MI(I_t) \cdot R1)[:, :, \frac{C'}{\tau}:] ) \cdot R2\]
其中 $R2 \in \mathbb{R}^{C' \times C}$ 是一个可学习的逆变换，将图像特征映射回原始嵌入空间。最后我们通过残差连接用 $F_t$ 更新 $MI(I_t)$ 。我们还零初始化 $R1, R2$ 和2D-Conv以平滑微调。

### 3.4 模态间时间建模

尽管维护短队列和采用通道移位能够有效地为图像特征聚合时间信息，但全局上下文是有限的。当对象非常大或者RGB-D相机停止移动时，这将导致性能下降。然而，如前所述，从流视频中提取丰富的全局上下文确实是内存和计算密集型的。为了解决这个问题，我们求助于点云记忆来提取全局上下文，因为点云特征有效地缓存在共享的3D空间中，因此队列的长度可以更长。我们设计了一个3D到2D适配器，以利用全局3D特征来细化当前图像特征。这里我们首先将 $mP_{t-1}$ 投影到离散图像坐标系中，这是 $S$ 的逆函数，然后转换为稀疏张量。通过这种方式，我们保持了点云记忆的稀疏性并使2D特征几何感知。然后在稀疏张量上应用2D稀疏卷积来聚合上下文信息，然后通过密集化操作将特征保持在图像内部，并将其他像素用零填充。最后我们将密集化的2D特征添加到 $MI(I_t) \cdot R1$ 以增强具有更丰富全局上下文的图像特征。我们零初始化2D稀疏卷积以平滑训练。为了获得最终的在线感知结果，需要一个预测融合策略。由于我们工作的重点是图像和点云主干的时间学习模块，我们只采用了一个简单的后处理策略来融合整个帧的预测，我们在第4.1节中详细介绍。

## 4. 实验

在本节中，我们首先描述了我们的数据集和实现细节。然后我们在房间级和在线基准测试中将我们的方法与最先进的方法进行比较，以全面分析基于记忆的适配器的优势。最后我们进行消融研究以验证我们设计的有效性。

### 4.1 基准和实现细节

我们在两个数据集上评估我们的方法：ScanNet[7]和SceneNN[15]。ScanNet包含1513个扫描场景序列，其中我们使用1201个序列进行训练，其余312个用于测试。SceneNN是一个较小的数据集，包含50个高质量扫描场景序列和语义标签。经过仔细筛选，我们选择了12个干净序列进行测试。我们在ScanNet上训练所有模型，并在ScanNet或SceneNN上评估它们。

基准测试：我们首先在房间级基准测试中比较不同方法，即在重建的完整场景上的性能。对于语义分割，我们在ScanNet和SceneNN上比较不同方法。由于在线方法可能不执行3D重建，我们将它们的预测通过最近邻插值映射到重建的点云上，以便与离线方法进行比较。对于对象检测和实例分割，度量是在每个对象上计算的，而不是在整个点云上。因此，我们使用重建的点云和RGB-D视频作为离线和在线方法的输入，并基于它们各自的输入计算指标。我们还遵循AnyView[44]的方法，在ScanNet上组织了一个在线基准测试，以便更全面地评估。我们将每个房间的RGB-D视频分成几个不重叠的序列，并将每个序列视为一个独立的场景，其中每个序列的数量或长度可以设置为不同的值。通过这种方式，我们可以衡量不同方法在输入场景不完整和不同规模时的泛化能力，这是一个更实际的设置。在我们的实验中，我们将每个房间分成1/5/10个序列或固定长度为5/10/15的序列，得到6个指标。实现细节：为了训练MSV，我们首先按照Pri3D[14]训练一个2D感知模型MI。我们使用UNet[37]进行语义分割，使用FasterRCNN[36]（只需要ResNet[12]和FPN[21]主干）进行对象检测和实例分割。然后我们固定图像主干并在ScanNet-25k[7]上训练MSV，这是一个单视图RGB-D数据集。对于在线感知，我们零初始化记忆基适配器并将它们插入到MSV中。然后我们在ScanNet的RGB-D视频上训练新模型。为了减少内存占用，我们在每次迭代中为每个场景随机采样8个相邻的RGB-D帧。我们将我们的记忆基适配器插入到主干和颈部之间。对于输出多级特征的主干，我们在不同级别插入不同的适配器。在超参数方面，我们设置l = 50, s = 2.5, τ = 8 和 δ = 0.03。我们简单地使用与原始论文中为离线训练设计的相同的优化器配置（用于训练模型）。对于预测融合，我们采用不同的策略针对不同任务。语义分割：每个帧的预测被串联起来，与 $S_t$ 具有相同数量的点。我们使用2cm的体素化来统一同一体素网格内点的预测，通过通道-wise最大池化。对象检测：每个帧预测的边界框通过3D NMS合并。当两个不同时间的框在NMS期间发生碰撞时，我们为更新帧中的框的分类分数添加δ。这是因为我们的方法确保主干为更新帧提取了更完整的几何特征。实例分割：实例分割可以分为基于变换器[40]、基于分组[17, 42]和基于检测[13, 18, 45]的。由于前两种需要为在线感知特别设计掩码融合策略[22]，我们选择基于检测的方式，我们可以首先进行在线3D对象检测，然后使用框来裁剪和分割存储在记忆中的点云特征。

### 4.2 与最先进方法的比较

我们将我们的方法与顶级的离线和在线3D感知模型进行比较。离线模型是指在第3.1节中描述的 $M_{Rec}$ ，它们在重建的点云上进行训练。带有后缀”-SV”的模型是指在单视图RGB-D图像上训练的 $M_{SV}$ 。房间级基准测试：默认情况下，离线方法处理重建的点云，在线方法处理姿势化的RGB-D视频，没有3D重建。特殊情况用†表示。注意在线方法与离线替代方案比较时存在挑战，因为离线方法直接处理房间的完整和清洁的3D几何，而在线方法处理部分和嘈杂的帧。根据表1和表2，通过简单地将记忆基适配器插入到MSV中，我们显著提高了它们在完整场景上的准确性，并与专门为每项任务设计的最先进的在线3D场景感知模型相比，实现了更好的性能。我们观察到在MSV上的性能提升特别是在3D对象检测和实例分割任务上尤为显著。这是因为这些任务需要对每个对象进行完整的预测，而通常很难用单个RGB-D帧推断出大型对象的整个几何形状。我们还注意到我们的方法甚至在语义分割任务上超过了离线方法。由于这项任务需要对局部几何进行更细致的感知，而不是全局上下文，我们的方法可以仅使用部分和嘈杂的输入预测更细粒度的分割。

在线基准测试：在这个基准测试中，所有方法的输入都是姿势化的RGB-D序列。我们为离线方法串联每个序列的RGB-D帧的点云。由于INS-Conv的代码不可获取，我们没有在该基准测试中与其进行比较。根据表3，离线方法在部分和嘈杂的场景上显示出较差的泛化能力，特别是当输入序列较短时。注意离线方法在每个时间处理整个观察到的场景 $S_t$ 。在处理 $S_{t+1}$ 时，提取的 $S_t$ 的特征被浪费。相反，在线方法每次处理单个帧 $x_t$ 并融合每帧的预测，这在实时机器人任务中更加高效和实用。配备了我们的记忆基适配器，MSV在所有任务和实验设置上都比其他离线和在线方法实现了最佳性能。我们观察到输入序列越长，与MSV相比的提升越大，这验证了我们的模块可以有效地聚合长期时间信息。我们在图5中可视化了不同方法的预测结果。可以看出，由于时间建模能力，我们的方法比MSV更准确，并且对帧数的鲁棒性比离线方法更好。

### 4.3 消融研究

我们首先在ScanNet的3D语义分割任务上消融两个记忆基适配器的设计选择。

点云和图像模块：表4验证了我们设计的有效性。我们观察到去除体素最大池化会显著降低性能，这表明更新记忆的重要性。随着s的增加，性能首先提高然后保持稳定甚至略有下降，这表明邻域上下文信息对时间学习很重要，但太大的邻域体素集合带来了太多冗余特征。大的s也会增加计算开销，因此我们选择s = 2.5以实现最佳准确性-计算权衡。我们观察到τ的影响与s类似，因此选择适当的值对高准确性和较少的内存存储都很重要。从这些实验中，我们还验证了”适配器范式”的有效性，包括残差连接、零初始化和插入后的主干。

固定的主干：在微调我们的适配器时，我们固定了图像主干和其他参数。我们进一步研究了当图像和点云主干在微调适配器时都固定时我们方法的性能。如图5所示，即使图像和点云主干都固定，我们的方法仍然在所有三个在线任务上实现了最先进的性能。通过这种方式，我们可以进一步减少内存占用和训练时间，为用户提供更多的效率-准确性权衡。

## 5. 结论

在本文中，我们介绍了在线3D场景感知的记忆基适配器。主流的3D场景感知方法是离线的，这在大多数实时应用中很难应用，其中只能访问流式的RGB-D视频。现有的在线感知方法为模型和任务特定的时间学习设计，但它们大多只关注单模态的时间聚合，因此无法充分利用图像和点云特征之间的时间关系。为此，我们提出了即插即用的时间学习模块，可以通过简单地插入记忆基适配器并在RGB-D视频上微调，赋予离线方法在线感知能力。具体来说，给定从主干提取的点云和图像特征，我们首先设计了一个排队记忆机制来缓存这些信息，并保持合理的存储开销。然后我们设计了聚合模块，它们直接在记忆上操作，并将缓存特征的时间信息传递给当前帧。由于图像特征的全局上下文受到短队列的限制，我们进一步提出了3D到2D适配器，以利用3D记忆增强图像特征。我们在ScanNet和SceneNN上进行了广泛的实验。通过为我们的模块配备离线模型，我们与最先进的在线方法相比，在三个场景感知任务上都实现了领先的性能，甚至无需任何模型和任务特定设计。





## 附录

A. 详细架构

我们在图6中展示了三个任务中图像和点云主干的架构，并展示了如何将记忆基适配器插入其中。对于在线3D语义分割，我们使用U-Net[37]作为图像主干，Minkowski-UNet[6]作为点云主干，分别在图6 (D)和(C)中展示。对于在线3D对象检测，我们采用ResNet[12]和FPN[21]作为图像主干，FCAF3D[38]作为点云主干，分别在图6 (E)和(B)中展示。对于在线3D实例分割，我们使用与对象检测任务相同的图像主干，并采用TD3D[18]作为点云主干，分别在图6 (E)和(A)中展示。注意，对于TD3D，主干保持了高分辨率的场景表示，用于ROI-wise实例预测。我们构建了一个点云记忆来缓存这个场景表示，这确保了每个ROI内的点云是到目前为止最新的。这种设计帮助我们通过简单地执行3D NMS来获得完整的实例掩码，避免了复杂的掩码融合策略[22]来合并不同帧的实例掩码。

B. 训练超参数

我们分两个阶段训练在线感知模型。首先我们在ScanNet25k[7]上训练单视图感知模型MSV。其次，我们将记忆基适配器插入到MSV中，并在ScanNet RGB-D视频上微调网络。对于在线语义分割，我们将第一阶段的最大周期设置为250，权重衰减设置为0.01，初始学习率设置为0.0008，并采用AdamW优化器和OneCycleLR调度器。然后在第二阶段，我们将最大周期设置为36，权重衰减设置为0.01，初始学习率设置为0.008，并采用AdamW优化器和在第24和32周期时进行步进的调度器。对于在线对象检测，我们在第一阶段将最大周期设置为12，权重衰减设置为0.0001，初始学习率设置为0.001，并采用AdamW优化器和在第8和11周期时进行步进的调度器。然后在第二阶段微调时，我们采用相同的超参数。对于在线实例分割，我们在第一阶段将最大周期设置为33，权重衰减设置为0.0001，初始学习率设置为0.001，并采用AdamW优化器和在第28和32周期时进行步进的调度器。然后在第二阶段微调时，我们采用相同的超参数。

C. 类别特定结果

我们提供了我们方法在三个3D场景感知任务上的类别特定实验结果。表6和7显示了ScanNet和SceneNN数据集上我们方法的3D语义分割结果，包括每个类别的IoU。表8和9显示了ScanNet数据集上我们方法的3D对象检测结果，包括每个类别的AP25和AP50。表10和11显示了ScanNet数据集上我们方法的3D实例分割结果，包括每个类别的AP25和AP50。